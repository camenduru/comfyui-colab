{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/comfyui-colab/blob/main/api/raemuXL_v35Lightning.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaAJk33ppFw1"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/camenduru/KaonashiUI\n",
        "%cd /content/KaonashiUI\n",
        "!pip install xformers==0.0.26.post1 torchsde==0.2.6 einops==0.8.0 diffusers==0.28.0 transformers==4.41.2 accelerate==0.30.1\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/TostModels/resolve/main/models/checkpoints/raemuXL_v35Lightning.safetensors -d /content/KaonashiUI/models/checkpoints -o model.safetensors\n",
        "import torch\n",
        "import random\n",
        "from kaonashi.sd import load_checkpoint_guess_config\n",
        "import nodes\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "with torch.inference_mode():\n",
        "    model_patcher, clip, vae, clipvision = load_checkpoint_guess_config(\"/content/KaonashiUI/models/checkpoints/model.safetensors\", output_vae=True, output_clip=True, embedding_directory=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    height = 1024 # @param {type:\"number\"}\n",
        "    width = 1024 # @param {type:\"number\"}\n",
        "    seed = 0 # @param {type:\"number\"}\n",
        "    steps = 9 # @param {type:\"number\"}\n",
        "    cfg = 2.0 # @param {type:\"number\"}\n",
        "    sampler_name = \"euler\" # @param {type:\"string\"}\n",
        "    scheduler = \"normal\" # @param {type:\"string\"}\n",
        "    latent = {\"samples\":torch.zeros([1, 4, height // 8, width // 8])}\n",
        "    prompt= \"(masterpiece, best quality, very aesthetic, ultra detailed), intricate details, 1girl, esdeath, akame ga kill!, blue eyes, blue hair, military uniform, hat, ice, sitting, sitting on throne, crossed legs, snow, white footwear, thigh boots,\" # @param {type:\"string\"}\n",
        "    cond, pooled = clip.encode_from_tokens(clip.tokenize(prompt), return_pooled=True)\n",
        "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "    negative_prompt = \"(worst quality, low quality, very displeasing, lowres), (interlocked fingers, badly drawn hands and fingers, anatomically incorrect hands), blurry, watermark,\" # @param {type:\"string\"}\n",
        "    n_cond, n_pooled = clip.encode_from_tokens(clip.tokenize(negative_prompt), return_pooled=True)\n",
        "    n_cond = [[n_cond, {\"pooled_output\": n_pooled}]]\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(seed)\n",
        "    sample = nodes.common_ksampler(model=model_patcher, seed=seed, steps=9, cfg=2.0, sampler_name=\"euler\", scheduler=\"normal\", positive=cond, negative=n_cond, latent=latent, denoise=1)\n",
        "    sample = sample[0][\"samples\"].to(torch.float16)\n",
        "    vae.first_stage_model.cuda()\n",
        "    decoded = vae.decode_tiled(sample).detach()\n",
        "Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "e593ac106456af50ce7af38f9671c411b49d6cd90f9b885e167f0f594e09038c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
